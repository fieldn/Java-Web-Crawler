------------------------
CS390 Web Crawler Project
Computer Science Department
Purdue University
-------------------------

-------------------------
Web Crawler
-------------------------

This web crawler searches through the Purdue Computer Science domain, 
retrieving all web pages, the best-matching image on each page, and a
description for each page as well, and stores all of that information in a 
MySQL database. All words on every page are also stored, for later searches
on the Search Engine. For parsing each web page, I uzed JSoup. My crawler
can fully crawl roughly 10,000 web pages in about 25 minutes.

------------------------
Search Engine
------------------------

My search engine takes queries of any size, runs them through the MySQL
database, and returns the pages with matching words. I implemented a page-
rank system in which pages are shown in decreasing order of the number of
words they match. For the search engine, I used nanoHTTPD to easily write
html using Java.
